{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30b0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import layers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a880e3",
   "metadata": {},
   "source": [
    "# Data location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592d0ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In data: ['formulas.txt', 'names.txt', 'test', 'train']\n",
      "In train: ['formula', 'formulas-90%.txt', 'name', 'names-90%.txt']\n",
      "In test: ['formula', 'formulas-10%.txt', 'name', 'names-10%.txt']\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"organic-formula-name\"\n",
    "\n",
    "dataset_directory = os.path.join(os.path.dirname(\"data\"), \"data\", data_folder)\n",
    "train_directory = os.path.join(dataset_directory, \"train\")\n",
    "test_directory = os.path.join(dataset_directory, \"test\")\n",
    "\n",
    "print(\"In data:\", os.listdir(dataset_directory))\n",
    "print(\"In train:\", os.listdir(train_directory))\n",
    "print(\"In test:\", os.listdir(test_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee3cb6",
   "metadata": {},
   "source": [
    "### Sample file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a82a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\organic-formula-name\\\\train\\\\formula\\\\xaa.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m sample_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(train_directory, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformula\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxaa.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msample_file\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread())\n",
      "File \u001B[1;32m~\\PycharmProjects\\quimify-classifier\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    279\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    280\u001B[0m     )\n\u001B[1;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data\\\\organic-formula-name\\\\train\\\\formula\\\\xaa.txt'"
     ]
    }
   ],
   "source": [
    "sample_file = os.path.join(train_directory, \"formula\", \"xaa.txt\")\n",
    "\n",
    "print(open(sample_file).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756c0db",
   "metadata": {},
   "source": [
    "# Data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 32 # Random seed for data shuffling and transformations\n",
    "validation_split = 0.2 # Proportion of train data used to validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data_source = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_directory,\n",
    "    subset = \"training\",\n",
    "    seed = seed,\n",
    "    validation_split = validation_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f060c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_validation_data_source = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_directory,\n",
    "    subset = \"validation\",\n",
    "    seed = seed,\n",
    "    validation_split = validation_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_data_source = tf.keras.utils.text_dataset_from_directory(\n",
    "    test_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa595445",
   "metadata": {},
   "source": [
    "# Data pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_standardization(input_data):\n",
    "    input_data = tf.strings.lower(input_data) # CH3-CH=CH-CH(NO2)Br -> ch3-ch=ch-ch(no2)br\n",
    "    input_data = tf.strings.regex_replace(input_data, \"[^a-zà-ú]\", ' ') # ch3-ch=ch-ch(no2)br -> ch  ch ch ch no  br\n",
    "    return tf.strings.regex_replace(input_data, \"\\s+\", ' ') # ch  ch ch ch no  br  -> ch ch ch ch no br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 2048 # Sets a boundary for len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 16 # Vectorized string's dimension\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    output_mode = \"int\",\n",
    "    max_tokens = max_features,\n",
    "    standardize = data_standardization,\n",
    "    output_sequence_length = sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960154cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a text-only dataset (without labels), then calls adapt\n",
    "train_text = raw_train_data_source.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073a5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieves a batch of 32 reviews and labels from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_data_source))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "\n",
    "print(\"Review:\", first_review)\n",
    "print(\"Label:\", raw_train_data_source.class_names[first_label])\n",
    "print(\"Vectorized review:\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size:\", len(vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e400c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary:\", sorted(vectorize_layer.get_vocabulary(), key = len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_source = raw_train_data_source.map(vectorize_text)\n",
    "validation_data_source = raw_validation_data_source.map(vectorize_text)\n",
    "test_data_source = raw_test_data_source.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5efbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE # ??\n",
    "\n",
    "train_data_source = train_data_source.cache().prefetch(buffer_size = AUTOTUNE)\n",
    "validation_data_source = validation_data_source.cache().prefetch(buffer_size = AUTOTUNE)\n",
    "test_data_source = test_data_source.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef8355",
   "metadata": {},
   "source": [
    "# Model creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d817ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16 # ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(256, activation = \"relu\"),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf238e",
   "metadata": {},
   "source": [
    "### Loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dce3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    metrics = tf.metrics.BinaryAccuracy(threshold = 0.0),\n",
    "    loss = losses.BinaryCrossentropy(from_logits = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c305297",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d28708",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "history = model.fit(\n",
    "    epochs = epochs,\n",
    "    x = train_data_source,\n",
    "    validation_data = validation_data_source,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892bb372",
   "metadata": {},
   "source": [
    "### Compiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation(\"sigmoid\")\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    optimizer = \"adam\", \n",
    "    metrics = [\"accuracy\"],\n",
    "    loss = losses.BinaryCrossentropy(from_logits = False), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305bb10",
   "metadata": {},
   "source": [
    "# Model evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e11c5",
   "metadata": {},
   "source": [
    "### Using test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f31b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data_source)\n",
    "\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f52fc7",
   "metadata": {},
   "source": [
    "### Using raw test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = export_model.evaluate(raw_test_data_source)\n",
    "\n",
    "print(\"Raw test loss:\", loss)\n",
    "print(\"Raw test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6abe3d6",
   "metadata": {},
   "source": [
    "### Accuracy graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "accuracy = history_dict[\"binary_accuracy\"]\n",
    "validation_accuracy = history_dict[\"val_binary_accuracy\"]\n",
    "loss = history_dict[\"loss\"]\n",
    "validation_loss = history_dict[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.plot(epochs, accuracy, \"bo\", label = \"Training accuracy\") # Blue dots\n",
    "plt.plot(epochs, validation_accuracy, \"b\", label = \"Validation accuracy\") # Blue line\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe64ce",
   "metadata": {},
   "source": [
    "### Loss graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, \"ro\", label = \"Training loss\") # Red dots\n",
    "plt.plot(epochs, validation_loss, \"r\", label = \"Validation loss\") # Red line\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f99029",
   "metadata": {},
   "source": [
    "# Model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"H3C-CH2\",\n",
    "    \"ChCh\",\n",
    "    \"CH3-CO-O-CH2-CH3\",\n",
    "    \"CH3-CH2-O-CH2-CH3\",\n",
    "    \"CH3-CH2-CH=CH-COOH\",\n",
    "    \"ch3chch2ch(ch2ch2ch3)cooh\",\n",
    "    \"ch3(Ch3)Chch2Ch(Ch3)Ch2Ch(Ch2Ch2Ch3)Ch3\",\n",
    "    \"benceno\",\n",
    "    \"2-cloropentanato\",\n",
    "    \"di 2-cloropentanil éter\",\n",
    "    \"2-bromo-2-cloropropano\",\n",
    "    \"metanoato de isopropilo\",\n",
    "    \"orto-difenilciclohexano\",\n",
    "    \"2-bromo-2-cloropropil yododecil éter\",\n",
    "    \"3-cloro-2-fluoro-hexa-1,3-dien-5-in-1-ona\",\n",
    "    \"4-amino-2,6,6-tricloro-7,7-difluoro-89-metil-3-nitro-1,1-diyodononaconta-1,3-dien-5-ona\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    percentage = export_model.predict([example])[0][0] * 100\n",
    "    print(\"Formula\" if percentage < 50 else \"Name\", \"(\" + \"%.2f\" % percentage + \" %):\", example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
